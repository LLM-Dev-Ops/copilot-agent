---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-copilot-agent-hpa
  namespace: llm-copilot
  labels:
    app: llm-copilot-agent
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-copilot-agent
  minReplicas: 3
  maxReplicas: 20
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 minutes
      policies:
        - type: Percent
          value: 50  # Scale down max 50% of pods at a time
          periodSeconds: 60
        - type: Pods
          value: 2  # Or max 2 pods at a time
          periodSeconds: 60
      selectPolicy: Min  # Use the policy that scales down the least
    scaleUp:
      stabilizationWindowSeconds: 60  # 1 minute
      policies:
        - type: Percent
          value: 100  # Scale up max 100% (double) at a time
          periodSeconds: 30
        - type: Pods
          value: 4  # Or max 4 pods at a time
          periodSeconds: 30
      selectPolicy: Max  # Use the policy that scales up the most
  metrics:
    # CPU-based scaling
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    # Memory-based scaling
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

    # Custom metrics (requires metrics server with custom metrics)
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: "1000"

    # External metrics (e.g., from Prometheus)
    - type: External
      external:
        metric:
          name: llm_copilot_queue_depth
          selector:
            matchLabels:
              app: llm-copilot-agent
        target:
          type: AverageValue
          averageValue: "100"
